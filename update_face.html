<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8">
  <title>C·∫≠p nh·∫≠t khu√¥n m·∫∑t</title>
  <style>
    body {
      background: #121212;
      color: white;
      font-family: Arial, sans-serif;
      text-align: center;
      padding-top: 20px;
    }
    #videoContainer { position: relative; display: inline-block; }
    #video {
      border-radius: 10px;
      background: #000;
      transform: scaleX(-1);
      object-fit: cover;
    }

    #overlay {
      position: absolute;
      top: 0;
      left: 0;
      pointer-events: none;
      width: 320px;
      height: 400px;
    }
    #captureBtn {
      margin-top: 15px;
      padding: 10px 20px;
      background-color: #0078D4;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
      font-size: 16px;
    }
  </style>
</head>
<body>
  <h2>C·∫≠p nh·∫≠t khu√¥n m·∫∑t</h2>
  <div id="videoContainer">
    <video id="video" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
  </div><br>
  <button id="captureBtn">üì∏ Ch·ª•p & c·∫≠p nh·∫≠t</button>

  <!-- Face API -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
<script>
const video = document.getElementById('video');
const overlay = document.getElementById('overlay');
const ctx = overlay.getContext('2d');

const DISPLAY_WIDTH = 320;
const DISPLAY_HEIGHT = 400;

overlay.width = DISPLAY_WIDTH;
overlay.height = DISPLAY_HEIGHT;

// üöÄ Load models
async function loadModels() {
  await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
  await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
  await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
  console.log("‚úÖ Models loaded");
  startCamera();
  detectLoop();
}

async function startCamera() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;
    await video.play();
    console.log("‚úÖ Camera started");
  } catch (err) {
    console.error("‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c camera:", err);
    alert("Kh√¥ng th·ªÉ truy c·∫≠p camera. H√£y b·∫≠t quy·ªÅn camera!");
  }
}

//V·∫Ω khung v√†ng ho·∫∑c xanh theo ƒë·ªô ·ªïn ƒë·ªãnh khu√¥n m·∫∑t
async function detectLoop() {
  const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 320 });
  let lastBox = null;
  let lostFrames = 0;

  setInterval(async () => {
    if (!video.videoWidth || !video.videoHeight) return;

    // Ph√°t hi·ªán khu√¥n m·∫∑t
    const result = await faceapi.detectSingleFace(video, options);

    ctx.clearRect(0, 0, overlay.width, overlay.height);

    // N·∫øu c√≥ k·∫øt qu·∫£ v√† box h·ª£p l·ªá
    if (result && result.box && typeof result.box.x === "number") {
      lostFrames = 0;
      lastBox = result.box;
    } else if (lastBox) {
      // Kh√¥ng ph√°t hi·ªán t·∫°m th·ªùi ‚Äî d√πng khung c≈© trong 10 khung h√¨nh (~2s)
      lostFrames++;
      if (lostFrames > 10) lastBox = null;
    }

    // V·∫Ω khung n·∫øu lastBox h·ª£p l·ªá
    if (lastBox && lastBox.width > 0 && lastBox.height > 0) {
      const padding = 0.25;
      const newX = Math.max(0, lastBox.x - lastBox.width * padding / 2);
      const newY = Math.max(0, lastBox.y - lastBox.height * padding / 2);
      const newW = lastBox.width * (1 + padding);
      const newH = lastBox.height * (1 + padding);

      const scaleX = DISPLAY_WIDTH / video.videoWidth;
      const scaleY = DISPLAY_HEIGHT / video.videoHeight;

      const mappedBox = {
        x: newX * scaleX,
        y: newY * scaleY,
        width: newW * scaleX,
        height: newH * scaleY
      };

      // L·∫≠t khung cho kh·ªõp video
      const mirroredX = DISPLAY_WIDTH - mappedBox.x - mappedBox.width;

      const centerX = mappedBox.x + mappedBox.width / 2;
      const centerY = mappedBox.y + mappedBox.height / 2;

      const stable =
        Math.abs(centerX - DISPLAY_WIDTH / 2) < 50 &&
        Math.abs(centerY - DISPLAY_HEIGHT / 2) < 50 &&
        mappedBox.width > 120 &&
        mappedBox.height > 120;

      ctx.lineWidth = 3;
      ctx.strokeStyle = stable ? "#00FF00" : "#FFD700";
      ctx.strokeRect(mirroredX, mappedBox.y, mappedBox.width, mappedBox.height);
    }
  }, 200);
}


// Khi click n√∫t ch·ª•p
document.getElementById("captureBtn").addEventListener("click", async () => {
  const EmployeeID = sessionStorage.getItem("employeeID");
  if (!EmployeeID) {
    alert("Kh√¥ng t√¨m th·∫•y th√¥ng tin nh√¢n vi√™n. H√£y ƒëƒÉng nh·∫≠p l·∫°i!");
    location.href = "login.html";
    return;
  }

  if (!video.srcObject) {
    alert("Camera ch∆∞a s·∫µn s√†ng.");
    return;
  }

  // Ph√°t hi·ªán khu√¥n m·∫∑t v√† t·∫°o descriptor
  const detection = await faceapi
    .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
    .withFaceLandmarks()
    .withFaceDescriptor();

  if (!detection) {
    alert("‚ùå Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t. H√£y th·ª≠ l·∫°i!");
    return;
  }

  const descriptor = Array.from(detection.descriptor);

  // G·ª≠i descriptor v·ªÅ server
  try {
    const response = await fetch("/api/update-face", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ EmployeeID, descriptor })
    });

    const result = await response.json();
    if (result.success) {
      alert("‚úÖ C·∫≠p nh·∫≠t khu√¥n m·∫∑t th√†nh c√¥ng!");
    } else {
      alert("‚ùå L·ªói: " + result.message);
    }
  } catch (err) {
    console.error("‚ùå Upload face failed:", err);
    alert("L·ªói khi g·ª≠i d·ªØ li·ªáu khu√¥n m·∫∑t l√™n server.");
  }
});

loadModels();
</script>

</body>
</html>
